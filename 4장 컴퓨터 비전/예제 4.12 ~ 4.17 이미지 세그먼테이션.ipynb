{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c46ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import SamProcessor, SamModel\n",
    "\n",
    "def filter_category(data):\n",
    "    # 16 = dog\n",
    "    # 23 = giraffe\n",
    "    return 16 in data[\"objects\"][\"category\"] or 23 in data[\"objects\"][\"category\"]\n",
    "\n",
    "def convert_image(data):\n",
    "    byte = io.BytesIO(data[\"image\"][\"bytes\"])\n",
    "    img = Image.open(byte)\n",
    "    return {\"img\": img}\n",
    "\n",
    "model_name = \"facebook/sam-vit-base\"\n",
    "processor = SamProcessor.from_pretrained(model_name) \n",
    "model = SamModel.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"s076923/coco-val\")\n",
    "filtered_dataset = dataset[\"validation\"].filter(filter_category)\n",
    "converted_dataset = filtered_dataset.map(convert_image, remove_columns=[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eece5f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def show_point_box(image, input_points, input_labels, input_boxes=None, marker_size=375):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    input_points = np.array(input_points)\n",
    "    input_labels = np.array(input_labels)\n",
    "\n",
    "    pos_points = input_points[input_labels[0] == 1]\n",
    "    neg_points = input_points[input_labels[0] == 0]\n",
    "    \n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25\n",
    "    )\n",
    "\n",
    "    if input_boxes is not None:\n",
    "        for box in input_boxes:\n",
    "            x0, y0 = box[0], box[1]\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            ax.add_patch(\n",
    "                plt.Rectangle(\n",
    "                    (x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2\n",
    "                )\n",
    "            )\n",
    "\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image = converted_dataset[0][\"img\"]\n",
    "input_points = [[[250, 200]]]\n",
    "input_labels = [[[1]]]\n",
    "\n",
    "show_point_box(image, input_points[0], input_labels[0])\n",
    "inputs = processor(\n",
    "    image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"input_points shape :\", inputs[\"input_points\"].shape)\n",
    "print(\"input_points :\", inputs[\"input_points\"])\n",
    "print(\"input_labels shape :\", inputs[\"input_labels\"].shape)\n",
    "print(\"input_labels :\", inputs[\"input_labels\"])\n",
    "print(\"pixel_values shape :\", inputs[\"pixel_values\"].shape)\n",
    "print(\"pixel_values :\", inputs[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116671c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_masks_on_image(raw_image, masks, scores):\n",
    "    if len(masks.shape) == 4:\n",
    "        masks = masks.squeeze()\n",
    "    if scores.shape[0] == 1:\n",
    "        scores = scores.squeeze()\n",
    "\n",
    "    nb_predictions = scores.shape[-1]\n",
    "    fig, axes = plt.subplots(1, nb_predictions, figsize=(30, 15))\n",
    "\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        mask = mask.cpu().detach()\n",
    "        axes[i].imshow(np.array(raw_image))\n",
    "        show_mask(mask, axes[i])\n",
    "        axes[i].title.set_text(f\"Mask {i+1}, Score: {score.item():.3f}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks.cpu(),\n",
    "    inputs[\"original_sizes\"].cpu(),\n",
    "    inputs[\"reshaped_input_sizes\"].cpu(),\n",
    ")\n",
    "\n",
    "show_masks_on_image(image, masks[0], outputs.iou_scores)\n",
    "print(\"iou_scores shape :\", outputs.iou_scores.shape)\n",
    "print(\"iou_scores :\", outputs.iou_scores)\n",
    "print(\"pred_masks shape :\", outputs.pred_masks.shape)\n",
    "print(\"pred_masks :\", outputs.pred_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64bc619",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_points = [[[250, 200], [15, 50]]]\n",
    "input_labels = [[[0, 1]]]\n",
    "input_boxes = [[[100, 100, 400, 600]]]\n",
    "\n",
    "show_point_box(image, input_points[0], input_labels[0], input_boxes[0])\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_points=input_points,\n",
    "    input_labels=input_labels,\n",
    "    input_boxes=input_boxes,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks.cpu(),\n",
    "    inputs[\"original_sizes\"].cpu(),\n",
    "    inputs[\"reshaped_input_sizes\"].cpu(),\n",
    ")\n",
    "\n",
    "show_masks_on_image(image, masks[0], outputs.iou_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe37f93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"mask-generation\", model=model_name)\n",
    "outputs = generator(image, points_per_batch=32)\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "ax = plt.gca()\n",
    "for mask in outputs[\"masks\"]:\n",
    "    show_mask(mask, ax=ax, random_color=True)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"outputs mask의 개수 :\", len(outputs[\"masks\"]))\n",
    "print(\"outputs scores의 개수 :\", len(outputs[\"scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277bdad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "detector = pipeline(\n",
    "    model=\"google/owlv2-base-patch16\", task=\"zero-shot-object-detection\"\n",
    ")\n",
    "\n",
    "image = converted_dataset[24][\"img\"]\n",
    "labels = [\"dog\", \"giraffe\"]\n",
    "results = detector(image, candidate_labels=labels, threshold=0.5)\n",
    "\n",
    "input_boxes = []\n",
    "for result in results:\n",
    "    input_boxes.append(\n",
    "        [\n",
    "            result[\"box\"][\"xmin\"],\n",
    "            result[\"box\"][\"ymin\"],\n",
    "            result[\"box\"][\"xmax\"],\n",
    "            result[\"box\"][\"ymax\"]\n",
    "        ]\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "inputs = processor(image, input_boxes=[input_boxes], return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks.cpu(),\n",
    "    inputs[\"original_sizes\"].cpu(),\n",
    "    inputs[\"reshaped_input_sizes\"].cpu()\n",
    ")\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "ax = plt.gca()\n",
    "\n",
    "for mask, iou in zip(masks[0], outputs.iou_scores[0]):\n",
    "    max_iou_idx = torch.argmax(iou)\n",
    "    best_mask = mask[max_iou_idx]\n",
    "    show_mask(best_mask, ax=ax, random_color=True)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
