{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22628136",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf62285",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # Windows\n",
    "# import pytesseract\n",
    "# pytesseract.pytesseract.tesseract_cmd = r\"<설치 경로>\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf6720",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # macOS\n",
    "# brew install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d808bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # Linux/Google Colab\n",
    "# sudo apt install tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6f204",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import LayoutLMv3FeatureExtractor\n",
    "\n",
    "def get_ocr_words_and_boxes(sample):\n",
    "    image_bytes = io.BytesIO(sample[\"image\"])\n",
    "    image = Image.open(image_bytes)\n",
    "\n",
    "    encoded_inputs = feature_extractor(image)\n",
    "    sample[\"words\"] = encoded_inputs.words[0]\n",
    "    sample[\"boxes\"] = encoded_inputs.boxes[0]\n",
    "    sample[\"pixel_values\"] = encoded_inputs.pixel_values[0]\n",
    "    return sample\n",
    "\n",
    "dataset = load_dataset(\"s076923/docvqa-train\")\n",
    "model_name = \"microsoft/layoutlmv3-base\"\n",
    "feature_extractor = LayoutLMv3FeatureExtractor(model_name)\n",
    "dataset_with_ocr = dataset[\"train\"].map(get_ocr_words_and_boxes)\n",
    "\n",
    "print(dataset_with_ocr[1].keys())\n",
    "print(\"question :\", dataset_with_ocr[1][\"question\"])\n",
    "print(\"answers :\", dataset_with_ocr[1][\"answers\"])\n",
    "print(\"words :\", dataset_with_ocr[1][\"words\"])\n",
    "print(\"boxes :\", dataset_with_ocr[1][\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26083c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def find_sublist(word_list, target_list):\n",
    "    word_list = [word.lower() for word in word_list]\n",
    "    target_list = target_list.lower().split()\n",
    "\n",
    "    for i in range(len(word_list) - len(target_list) + 1):\n",
    "        if word_list[i : i + len(target_list)] == target_list:\n",
    "            return target_list, i, i + len(target_list) - 1\n",
    "    return None, 0, 0\n",
    "\n",
    "question = dataset_with_ocr[10][\"question\"]\n",
    "words = dataset_with_ocr[10][\"words\"]\n",
    "answers = dataset_with_ocr[10][\"answers\"]\n",
    "print(question)\n",
    "print(words)\n",
    "print(answers)\n",
    "print()\n",
    "\n",
    "for answer in answers:\n",
    "    match, word_idx_start, word_idx_end = find_sublist(words, answer)\n",
    "    print(\"Match :\", match)\n",
    "    print(\"Word idx start :\", word_idx_start)\n",
    "    print(\"Word idx end :\", word_idx_end)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842dce7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3TokenizerFast\n",
    "from datasets import Features, Sequence, Value, Array2D, Array3D\n",
    "\n",
    "def find_answer_match(words, answers):\n",
    "    for answer in answers:\n",
    "        match, word_idx_start, word_idx_end = find_sublist(words, answer)\n",
    "        if match:\n",
    "            return match, word_idx_start, word_idx_end\n",
    "\n",
    "    for answer in answers:\n",
    "        for i in range(len(answer)):\n",
    "            answer_modified = answer[:i] + answer[i + 1 :]\n",
    "            match, word_idx_start, word_idx_end = find_sublist(words, answer_modified)\n",
    "            if match:\n",
    "                return match, word_idx_start, word_idx_end\n",
    "\n",
    "    return False, None, None\n",
    "\n",
    "def encode_dataset(examples, processor, max_length=512):\n",
    "    encoding = processor(\n",
    "        examples[\"question\"],\n",
    "        examples[\"words\"],\n",
    "        examples[\"boxes\"],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    cls_index = encoding.input_ids.index(processor.cls_token_id)\n",
    "    start_position = end_position = cls_index\n",
    "\n",
    "    match, word_idx_start, word_idx_end = find_answer_match(\n",
    "        examples[\"words\"], examples[\"answers\"]\n",
    "    )\n",
    "\n",
    "    if match:\n",
    "        sequence_ids = encoding.sequence_ids(0)\n",
    "        token_start_index = next(i for i, seq_id in enumerate(sequence_ids) if seq_id == 1)\n",
    "\n",
    "        token_end_index = len(encoding.input_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "        word_ids = encoding.word_ids()[token_start_index : token_end_index + 1]\n",
    "\n",
    "        start_position = token_start_index + word_ids.index(word_idx_start)\n",
    "        end_position = token_end_index - word_ids[::-1].index(word_idx_end)\n",
    "\n",
    "    encoding[\"image\"] = examples[\"pixel_values\"]\n",
    "    encoding[\"start_positions\"] = start_position\n",
    "    encoding[\"end_positions\"] = end_position\n",
    "    return encoding\n",
    "\n",
    "processor = LayoutLMv3TokenizerFast.from_pretrained(model_name)\n",
    "encoded_dataset = dataset_with_ocr.map(\n",
    "    lambda x: encode_dataset(x, processor),\n",
    "    remove_columns=dataset_with_ocr.column_names,\n",
    "    features=Features(\n",
    "        {\n",
    "            \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "            \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "            \"attention_mask\": Sequence(Value(dtype=\"int64\")),\n",
    "            \"image\": Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "            \"start_positions\": Value(dtype=\"int64\"),\n",
    "            \"end_positions\": Value(dtype=\"int64\")\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744922d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import LayoutLMv3ForQuestionAnswering\n",
    "\n",
    "model = LayoutLMv3ForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"DocVQA\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453ba0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LayoutLMv3Processor\n",
    "\n",
    "index = 5\n",
    "processor = LayoutLMv3Processor.from_pretrained(model_name)\n",
    "\n",
    "image_bytes = io.BytesIO(dataset_with_ocr[index][\"image\"])\n",
    "image = Image.open(image_bytes)\n",
    "\n",
    "full_text = processor.decode(encoded_dataset[\"input_ids\"][index])\n",
    "print(\"Full text:\", full_text)\n",
    "\n",
    "question = dataset_with_ocr[index][\"question\"]\n",
    "print(\"Question:\", question)\n",
    "\n",
    "start_position = encoded_dataset[\"start_positions\"][index]\n",
    "end_position = encoded_dataset[\"end_positions\"][index]\n",
    "answer = processor.decode(\n",
    "    encoded_dataset[\"input_ids\"][index][start_position : end_position + 1]\n",
    ")\n",
    "print(\"Answer:\", answer)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoded_inputs = processor(image, question, return_tensors=\"pt\")\n",
    "encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
    "print(\"Encoded input keys:\", encoded_inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dbe71",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "\n",
    "start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "start_index = start_logits.argmax(-1).item()\n",
    "end_index = end_logits.argmax(-1).item()\n",
    "predicted_answer = processor.decode(\n",
    "    encoded_inputs[\"input_ids\"].squeeze()[start_index : end_index + 1]\n",
    ")\n",
    "\n",
    "print(\"Predicted start_index:\", start_index)\n",
    "print(\"Predicted end_index:\", end_index)\n",
    "print(\"predicted_answer:\", predicted_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
